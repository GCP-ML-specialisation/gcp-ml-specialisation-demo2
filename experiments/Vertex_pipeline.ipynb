{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CAMNG3\\AppData\\Local\\Temp\\ipykernel_24784\\2628510963.py:7: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import compiler\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "from google.cloud import storage  # noqa: F401\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (component,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics, \n",
    "                        OutputPath,\n",
    "                        InputPath)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalise Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CAMNG3\\AppData\\Local\\Temp\\ipykernel_24784\\1610128380.py:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n",
      "  @component(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\", \"numpy\"],\n",
    "    output_component_file=\"pre_processing_data.yaml\", \n",
    "    base_image=\"python:3.9\")\n",
    "def pre_processed_bf_data(\n",
    "    train_name: str,\n",
    "    test_name: str,\n",
    "    BUCKET_URI: str,\n",
    "    raw_folder: str,\n",
    "    pre_processed_folder: str\n",
    "    ) -> str:\n",
    "    \n",
    "    import pandas as pd\n",
    "    from pre_processing_modules import basic_preprocessing, feature_engineering\n",
    "    from data_utils import read_csv_GCS, write_csv_GCS\n",
    "\n",
    "\n",
    "    # Define the bucket URI\n",
    "    df_train_path = BUCKET_URI + raw_folder + train_name\n",
    "    df_test_path = BUCKET_URI + raw_folder + test_name\n",
    "\n",
    "    df_train, df_test = read_csv_GCS(df_train_path, df_test_path)\n",
    "\n",
    "    df_train, df_test = basic_preprocessing(df_train, df_test)\n",
    "    df_train, df_test = feature_engineering(df_train, df_test)\n",
    "\n",
    "    write_csv_GCS(df_train, df_test,BUCKET_URI, pre_processed_folder, train_name, test_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"intro-pipeline-unique\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline( train_name: str = \"train.csv\",\n",
    "    test_name: str = \"test.csv\",\n",
    "    BUCKET_URI: str = BUCKET_URI,\n",
    "    raw_folder: str = \"raw_data/\",\n",
    "    pre_processed_folder: str = \"pre_processed_data/\"):\n",
    "    \n",
    "    pre_processed_bf_data(\n",
    "        train_name=train_name,\n",
    "        test_name=test_name,\n",
    "        BUCKET_URI=BUCKET_URI,\n",
    "        raw_folder=raw_folder,\n",
    "        pre_processed_folder=pre_processed_folder\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_name\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_name' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"bf_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west2/pipelines/runs/intro-pipeline-unique-20240624180815?project=573415920784\n",
      "PipelineJob projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815 current state:\n",
      "2\n",
      "PipelineJob projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815 current state:\n",
      "3\n",
      "PipelineJob projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815 current state:\n",
      "3\n",
      "PipelineJob projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815 current state:\n",
      "3\n",
      "PipelineJob projects/573415920784/locations/europe-west2/pipelineJobs/intro-pipeline-unique-20240624180815 current state:\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [pre-processed-bf-data].; Job (project_id = prj-dev-mlbf-flt-01-29e5, job_id = 3244964477799497728) is failed due to the above error.; Failed to handle the job: {project_number = 573415920784, job_id = 3244964477799497728}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m aip\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mPROJECT_ID, location\u001b[38;5;241m=\u001b[39mREGION)\n\u001b[0;32m     11\u001b[0m job \u001b[38;5;241m=\u001b[39m aip\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[0;32m     12\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mDISPLAY_NAME,\n\u001b[0;32m     13\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf_pipeline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     pipeline_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_URI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mpipeline-root/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:331\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[1;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    329\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[0;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:374\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[1;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    368\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[0;32m    369\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m    370\u001b[0m     reserved_ip_ranges\u001b[38;5;241m=\u001b[39mreserved_ip_ranges,\n\u001b[0;32m    371\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[0;32m    372\u001b[0m )\n\u001b[1;32m--> 374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:744\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [pre-processed-bf-data].; Job (project_id = prj-dev-mlbf-flt-01-29e5, job_id = 3244964477799497728) is failed due to the above error.; Failed to handle the job: {project_number = 573415920784, job_id = 3244964477799497728}\"\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "PROJECT_ID = \"prj-dev-mlbf-flt-01-29e5\"\n",
    "BUCKET_URI = \"gs://pipeline_black_friday/\"\n",
    "REGION = \"europe-west2\"\n",
    "DISPLAY_NAME = \"bf_pipeline_job_unique\"\n",
    "\n",
    "\n",
    "aip.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"bf_pipeline.json\",\n",
    "    pipeline_root=f\"{BUCKET_URI}pipeline-root/\",\n",
    ")\n",
    "\n",
    "job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\kfp\\dsl\\component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@component(packages_to_install=[\"google-cloud-storage\"])\n",
    "def two_outputs(\n",
    "    text: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"output_one\", str),  # Return parameters\n",
    "        (\"output_two\", str),\n",
    "    ]\n",
    "):\n",
    "    o1 = f\"output one from text: {text}\"\n",
    "    o2 = f\"output two from text: {text}\"\n",
    "    print(\"output one: {}; output_two: {}\".format(o1, o2))\n",
    "    return (o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def consumer(text1: str, text2: str, text3: str) -> str:\n",
    "    print(f\"text1: {text1}; text2: {text2}; text3: {text3}\")\n",
    "    return f\"text1: {text1}; text2: {text2}; text3: {text3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a pipeline that uses the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/intro-pipeline-unique-20240618112329?project=573415920784\n",
      "PipelineJob projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329 current state:\n",
      "2\n",
      "PipelineJob projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329 current state:\n",
      "3\n",
      "PipelineJob projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329 current state:\n",
      "3\n",
      "PipelineJob projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329 current state:\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [consumer].; Job (project_id = prj-dev-mlbf-flt-01-29e5, job_id = 8695161428190953472) is failed due to the above error.; Failed to handle the job: {project_number = 573415920784, job_id = 8695161428190953472}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m DISPLAY_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintro_pipeline_job_unique\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m job \u001b[38;5;241m=\u001b[39m aip\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[0;32m      4\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mDISPLAY_NAME,\n\u001b[0;32m      5\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintro_pipeline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     pipeline_root\u001b[38;5;241m=\u001b[39mPIPELINE_ROOT\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:331\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[1;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    329\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[0;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:374\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[1;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    368\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[0;32m    369\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m    370\u001b[0m     reserved_ip_ranges\u001b[38;5;241m=\u001b[39mreserved_ip_ranges,\n\u001b[0;32m    371\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[0;32m    372\u001b[0m )\n\u001b[1;32m--> 374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[1;32mc:\\Users\\CAMNG3\\OneDrive - PA Consulting Group\\GCP_ML_Specialisation\\Black_friday_forecasting_Pipeline\\env\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:744\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    746\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [consumer].; Job (project_id = prj-dev-mlbf-flt-01-29e5, job_id = 8695161428190953472) is failed due to the above error.; Failed to handle the job: {project_number = 573415920784, job_id = 8695161428190953472}\"\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"intro_pipeline_job_unique\"\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"intro_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    "\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting PipelineJob : projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329\n",
      "Delete PipelineJob  backing LRO: projects/573415920784/locations/us-central1/operations/8122597379456106496\n",
      "PipelineJob deleted. . Resource name: projects/573415920784/locations/us-central1/pipelineJobs/intro-pipeline-unique-20240618112329\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
