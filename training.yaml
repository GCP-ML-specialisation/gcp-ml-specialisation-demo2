# PIPELINE DEFINITION
# Name: training-hyperp-tuning
# Inputs:
#    df_train: system.Dataset
# Outputs:
#    trained_model: system.Model
components:
  comp-training-hyperp-tuning:
    executorLabel: exec-training-hyperp-tuning
    inputDefinitions:
      artifacts:
        df_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        trained_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-training-hyperp-tuning:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - training_hyperp_tuning
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'gcsfs'\
          \ 'numpy==1.23.5' 'scikit-learn==1.3.0' 'joblib' 'scipy' && \"$0\" \"$@\"\
          \n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef training_hyperp_tuning(\n    df_train: Input[Dataset],\n    trained_model:\
          \ Output[Model],\n):\n\n    import pandas as pd\n    import os\n    import\
          \ joblib\n    from sklearn.ensemble import RandomForestRegressor\n    from\
          \ sklearn.model_selection import (\n        RandomizedSearchCV,\n      \
          \  StratifiedKFold,\n    )\n    from scipy.stats import randint, uniform\n\
          \n    df_train = pd.read_csv(df_train.path + \".csv\")\n\n    x = df_train.drop(\"\
          Purchase\", axis=1)\n    y = df_train[\"Purchase\"]\n\n    regressor = RandomForestRegressor(n_estimators=10,\
          \ random_state=42, oob_score=True)\n\n    # Define the parameter distributions\n\
          \    param_dist = {\n        \"n_estimators\": randint(50, 500),  # Number\
          \ of trees in the forest\n        \"max_features\": [\n            \"auto\"\
          ,\n            \"sqrt\",\n            \"log2\",\n        ],  # Number of\
          \ features to consider at each split\n        \"max_depth\": randint(10,\
          \ 100),  # Maximum depth of the tree\n        \"min_samples_split\": randint(\n\
          \            2, 20\n        ),  # Minimum number of samples required to\
          \ split a node\n        \"min_samples_leaf\": randint(\n            1, 20\n\
          \        ),  # Minimum number of samples required to be at a leaf node\n\
          \        \"bootstrap\": [\n            True,\n            False,\n     \
          \   ],  # Whether bootstrap samples are used when building trees\n     \
          \   \"min_impurity_decrease\": uniform(\n            0, 0.1\n        ),\
          \  # Threshold for early stopping in tree growth\n    }\n\n    param_comb\
          \ = 20\n\n    random_search = RandomizedSearchCV(\n        regressor,\n\
          \        param_distributions=param_dist,\n        n_iter=param_comb,\n \
          \       scoring=\"neg_root_mean_squared_error\",\n        cv=5,\n      \
          \  verbose=4,\n        random_state=42,\n    )\n\n    random_search.fit(x,\
          \ y)\n    regressor_best = random_search.best_estimator_\n\n    trained_model.metadata[\"\
          framework\"] = \"RandomForestRegressor\"\n    os.makedirs(trained_model.path,\
          \ exist_ok=True)\n    joblib.dump(regressor_best, os.path.join(trained_model.path,\
          \ \"model.joblib\"))\n\n"
        image: python:3.10
pipelineInfo:
  name: training-hyperp-tuning
root:
  dag:
    outputs:
      artifacts:
        trained_model:
          artifactSelectors:
          - outputArtifactKey: trained_model
            producerSubtask: training-hyperp-tuning
    tasks:
      training-hyperp-tuning:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-training-hyperp-tuning
        inputs:
          artifacts:
            df_train:
              componentInputArtifact: df_train
        taskInfo:
          name: training-hyperp-tuning
  inputDefinitions:
    artifacts:
      df_train:
        artifactType:
          schemaTitle: system.Dataset
          schemaVersion: 0.0.1
  outputDefinitions:
    artifacts:
      trained_model:
        artifactType:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
